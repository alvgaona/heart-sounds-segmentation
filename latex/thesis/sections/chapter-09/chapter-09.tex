\chapter{Conclusiones generales}

\indent Este trabajo muestra que la correcta selección de atributos adecuado, acompañado de un modelo relativamente
simple en la etapa de clasificación, alcanza para obtener una efectividad muy cercana a la del estado del arte. En
el cuadro \ref{tab:performance-comparison} se ve que, entre los cuatro algoritmos más famosos en la segmentación de
\acrshort{pcg}, el de F. Renna y M. Coimbra \cite{pp:renna2018} obtiene las mejores métricas. Éste se basa en el
mismo preprocesamiento (acondicionamiento de la señal) y extracción de atributos de los \acrshort{pcg} que Springer
\textit{et al.} \cite{pp:springer2015}, el cual utiliza los métodos propuestos por Schmidt \cite{pp:schmidt2010}
mejorando algunos de los algoritmos. \\
\indent El presente trabajo utiliza, una DNN con una arquitectura muy simple.
Ésta, se ha visto, que contiene sólo una capa recurrente \acrshort{lstm} y diferentes capas intermedias que ayudan a
la clasificación. De esta manera, se alcanzan métricas muy cercanas al estado del arte con una buena elección de
atributos. En cuanto a la exactitud ($Acc$) queda en segundo lugar pero carece de eficiencia en el resto de las
métricas, precisión ($P_+$) y sensibilidad $Se$. \\
\indent La arquitectura de la red \acrshort{lstm} es relativamente simple en comparación con otras arquitecturas,
por ejemplo U-Net, de caracter convolucional, donde se aplican mayores grados de profundidad en cuanto a las capas.
Por otro lado, posee antecedentes en cuanto a la aplicación de distintos problemas de segmentación. Un ejemplo es el
conteo de células, detección y morfometría por medio de imágenes.

\section{Tiempo de procesamiento}

\indent Dado que \textit{Deep learning} se encuentra basado en conexionismo: cuando una neurona o unidad en un
modelo de \textit{Machine learning} no es inteligente, un conjunto de neuronas pueden demostrar un comportamiento lo
suficientemente inteligente para ciertas aplicaciones. Es importante enfatizar que el número de neuronas debe ser
grande para realizar tareas complejas. El tamaño de las redes neuronales han crecido exponencialmente, aunque se
comparan con el tamaño del sistema nervioso central de insectos. Por lo tanto, debido a que el tamaño de las redes
neuronales es crítico, requiere un alto desempeño en cuanto a hardware y software. \\
\indent En definitiva la implementación de los algoritmos, el hardware y el contexto en el que se desea llevar la
aplicación imponen restricciones en el tiempo de procesamiento.

\subsection*{Implementaciones en CPU}

\indent En un principio el entrenamiento y la evaluación de las redes neuronales se diseñaban para un único CPU. Hoy
en día esto no es suficiente. En este trabajo no se prioriza el tiempo de procesamiento, con lo cual la etapa de
entrenamiento que depende de muchos factores, como han de ser la cantidad de datos de entrada y la arquitectura de
la red. Generalmente, la mayoría de estos modelos son utilizados en GPU. Sin embargo, con un cuidado
desarrollo e implementación se pueden lograr mejores tiempos de ejecución con implementaciones en CPU.

\subsection*{Implementaciones en GPU}

\indent Para el desarrollo de los algoritmos de clasificación, por ejemplo de este trabajo y para la implementación
en tiempo real, utilizar GPU acelera el tiempo de procesamiento. Las GPU, hardware especializado que permite
realizar multiplicación de matrices y división en paralelo, se diferencian de las CPU que para realizar tareas en
paralelo es necesario utilizar lo que se conoce como \textit{branching}.

\subsection*{Compresión}

\indent La compresión del modelo es una ventaja ante la limitante de memoria y tiempos de lectura y evaluación. Es
el caso de utilizar la idea de este trabajo en una suerte de producto en tiempo real donde se considera el tiempo de
evaluación mucho más importante que el tiempo de entrenamiento. Esto se debe a que el desarrollador tiene mucho más
recursos para realizar las etapas de entrenamiento y evaluación. Asimismmo, aunque sólo sea necesario entrenar el
modelo una única vez e implementarlo para su uso, el usuario cuenta con cómputo más barato y menos performante.
Aquí, es cuando la compresión del modelo juega un papel muy importante a la hora de llevar a producción una
implementación.

\section{Futuras líneas de trabajo}

\subsection*{Nuevas bases de datos}

\indent La base de datos es un asunto bastante crítico en cuanto a la generalización de los modelos. Aumentar la
cantidad de registros de fonocardiograma con sus anotaciones asociadas es un factor clave para desarrollar aún más
la propuesta de este trabajo y muchos otros también. Junto al dispositivo que el \acrshort{iam} del
\acrshort{conicet} se encuentra desarrollando, será posible acceder a adquisiciones de nuevas señales. Por otro
lado, ligado con la arquitectura de la red, sería una buena línea de trabajo obtener una base de datos basadas en
imágenes tiempo-frecuencia \footnote{Algunos ejemplos de transformaciones que logran esto son la \acrshort{stft},
\acrshort{fsst}, \acrshort{cwt}/\acrshort{dwt}}.

\subsection*{Mejoras en la arquitectura}

\indent La arquitectura es una de las principales limitaciones que resultan de este trabajo. En la arquitectura
presentada sólo consta con un único nivel de profundidad, lo que hace una primera versión de arquitectura muy simple
pero con un gran desempeño. Esta genera la pregunta de cuál sería la eficiencia de un modelo recurrente con una
profundidad mucho mayor. \\
\indent Una buena línea de investigación, para obtener mejores métricas, es la arquitectura donde sería bueno
aumentar la profundidad del modelo con otras capas recurrentes o convolucionales. Para lo último es necesario
aplicar transformaciones a los tensores para que sean compatibles entre sí. Distintos trabajos han demostrado
conseguir eficiencias altas gracias a la complejidad de los modelos. Casos como las arquitecturas de AlexNet
\cite{pp:alexnet} y GoogleNet \cite{pp:googlenet} han mejorado distintas aplicaciones como son aplicaciones de
\textit{Computer Vision} y reconocimiento de imágenes, entre otras.

\subsection*{Complejidad algorítmica}

\indent La complejidad algorítmica de las redes neuronales es un tema de discusión que se encuentra ligada
generalmente al algoritmo utilizado para resolver la estimación de los parámetros de la arquitectura. Algunos
ejemplos de estos algoritmos son \textit{Adam} y \textit{RMSProp}. Por supuesto que además depende de la profundidad
y de la naturaleza de las redes. Por ende, ésta es una línea de trabajo importante para definir los tiempos de
entrenamiento y evaluación del modelo.

\subsection*{Segmentación en tiempo real}

\indent Después de tener definido varios de los problemas mencionados anteriormente, es factible pensar en una
implementación de segmentación en tiempo real. Los temas más críticos para llevar a cabo esto son la elección de la
implementación de la red (bajo CPU o GPU), la arquitectura y la complejidad algorítmica y los tiempos asociados. \\
\indent La segmentación en tiempo real requiere definir lo que se conoce como \textit{pipeline} donde a partir de un
\textit{stream} de datos que ingresa al mismo y se realizan distintas etapas de procesamiento (acondicionamiento,
preprocesamiento, procesamiento, post-procesamiento). Para ellos es necesario bajo ciertos requerimientos del
segmentador online definir los tiempos alcanzables en cada instancia del \textit{pipeline}. Esta no es una tarea
trivial y requiere una investigación previa o acompañada de las distintas líneas de trabajo antes mencionada.
